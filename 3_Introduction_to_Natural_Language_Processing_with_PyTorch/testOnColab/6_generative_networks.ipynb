{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative networks\n",
        "\n",
        "Recurrent Neural Networks (RNNs) and their gated cell variants such as Long Short Term Memory Cells (LSTMs) and Gated Recurrent Units (GRUs) provided a mechanism for language modeling, i.e. they can learn word ordering and provide predictions for next word in a sequence. This allows us to use RNNs for **generative tasks**, such as ordinary text generation, machine translation, and even image captioning.\n",
        "\n",
        "In RNN architecture we discussed in the previous unit, each RNN unit produced next next hidden state as an output. However, we can also add another output to each recurrent unit, which would allow us to output a **sequence** (which is equal in length to the original sequence). Moreover, we can use RNN units that do not accept an input at each step, and just take some initial state vector, and then produce a sequence of outputs.\n",
        "\n",
        "This allows for different neural architectures that are shown in the picture below:\n",
        "\n",
        "<img alt=\"Image showing common recurrent neural network patterns.\" src=\"images/6-generative-networks-1.jpg\" align=\"middle\" />\n",
        "*Image from blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\" by Andrej Karpaty*\n",
        "\n",
        "* **One-to-one** is a traditional neural network with one input and one output\n",
        "* **One-to-many** is a generative architecture that accepts one input value, and generates a sequence of output values. For example, if we want to train `image captioning` network that would produce a textual description of a picture, we can have a picture as input, pass it through CNN to obtain hidden state, and then have recurrent chain generate caption word-by-word\n",
        "* **Many-to-one** corresponds to RNN architectures we described in the previous unit, such as `text classification`\n",
        "* **Many-to-many**, or **sequence-to-sequence** corresponds to tasks such as `machine translation` or `language translation`, where we have first RNN collect all information from the input sequence into the hidden state, and another RNN chain unrolls this state into the output sequence.\n",
        "\n",
        "In this unit, we will focus on simple *generative models* that help us generate text. For simplicity, let's build a **character-level network**, which generates news articles text letter by letter. While generating news articles may seem quite impractical, the same idea of sequence generation is used in many practical tasks, such as machine translation, text summarization, etc.\n",
        "\n",
        "During training, we need to take some text corpus, and split it into letter sequences. "
      ],
      "metadata": {
        "id": "3TSxTZyIiY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt\n",
        "!pip install torchinfo\n",
        "!wget -q https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/torchnlp.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ7MLNvVnAb9",
        "outputId": "195860d7-de23-44f4-e158-0c9866062432"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.3\n",
            "  Downloading gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface==0.0.1\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.2.2)\n",
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.5.1.48\n",
            "  Downloading opencv_python-4.5.1.48-cp38-cp38-manylinux2014_x86_64.whl (50.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow==7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 9)) (1.7.3)\n",
            "Collecting torch==1.8.1\n",
            "  Downloading torch-1.8.1-cp38-cp38-manylinux1_x86_64.whl (804.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.1/804.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "  Downloading torchaudio-0.8.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchinfo==0.0.8\n",
            "  Downloading torchinfo-0.0.8-py3-none-any.whl (16 kB)\n",
            "Collecting torchtext==0.9.1\n",
            "  Downloading torchtext-0.9.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.9.1\n",
            "  Downloading torchvision-0.9.1-cp38-cp38-manylinux1_x86_64.whl (17.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.3.3\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk==3.5->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 10)) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.25.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 15)) (3.9.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.1->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 13)) (4.0.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434692 sha256=f5e94f15ea7522c751ec6176583a21ab36d1538c0c1350c7ad273d56d590605a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=39e4d67013f886a983c7a95260c024a9b364a6cdb851d4bf65da968ef0182980\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: tokenizers, huggingface, torchinfo, sacremoses, numpy, nltk, transformers, torch, opencv-python, torchvision, torchtext, torchaudio, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.1\n",
            "    Uninstalling torchtext-0.14.1:\n",
            "      Successfully uninstalled torchtext-0.14.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1+cu116\n",
            "    Uninstalling torchaudio-0.13.1+cu116:\n",
            "      Successfully uninstalled torchaudio-0.13.1+cu116\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-3.8.3 huggingface-0.0.1 nltk-3.5 numpy-1.18.5 opencv-python-4.5.1.48 sacremoses-0.0.53 tokenizers-0.10.3 torch-1.8.1 torchaudio-0.8.1 torchinfo-0.0.8 torchtext-0.9.1 torchvision-0.9.1 transformers-4.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.8/dist-packages (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from torchnlp import *\n",
        "train_dataset,test_dataset,classes,vocab = load_dataset()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train.csv: 29.5MB [00:00, 87.6MB/s]\n",
            "test.csv: 1.86MB [00:00, 80.2MB/s]                  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocab...\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "b-VSkAodiY67",
        "outputId": "726d46f1-5cc7-4430-b818-1caeb5a312d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building character vocabulary\n",
        "\n",
        "To build character-level generative network, we need to split text into individual characters instead of words. This can be done by defining a different tokenizer.  For example:\n",
        "- `vocab.stoi` - maps token strings in the vacab to a numeric identifier\n",
        "- `vocab.itos` - maps a number index from the vocab to a string "
      ],
      "metadata": {
        "id": "lOGXVI4ziY68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def char_tokenizer(words):\n",
        "    return list(words) #[word for word in words]\n",
        "\n",
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(char_tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size = {vocab_size}\")\n",
        "print(f\"Encoding of 'a' is {vocab.stoi['a']}\")\n",
        "print(f\"Character with code 13 is {vocab.itos[13]}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size = 84\n",
            "Encoding of 'a' is 4\n",
            "Character with code 13 is h\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "bWyeV3O_iY68",
        "outputId": "6d4dcbed-e7b9-4ba4-a0f1-d3fc8b82e5ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(84):\n",
        "    print(f\"Encoding of '{vocab.itos[i]}' is {vocab.stoi[vocab.itos[i]]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJgrghA1nwll",
        "outputId": "e2d6b750-6d9e-4eee-aba0-7a9e2a1ac9a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding of '<unk>' is 0\n",
            "Encoding of '<pad>' is 1\n",
            "Encoding of ' ' is 2\n",
            "Encoding of 'e' is 3\n",
            "Encoding of 'a' is 4\n",
            "Encoding of 't' is 5\n",
            "Encoding of 'o' is 6\n",
            "Encoding of 'i' is 7\n",
            "Encoding of 'n' is 8\n",
            "Encoding of 's' is 9\n",
            "Encoding of 'r' is 10\n",
            "Encoding of 'l' is 11\n",
            "Encoding of 'd' is 12\n",
            "Encoding of 'h' is 13\n",
            "Encoding of 'c' is 14\n",
            "Encoding of 'u' is 15\n",
            "Encoding of 'p' is 16\n",
            "Encoding of 'm' is 17\n",
            "Encoding of 'g' is 18\n",
            "Encoding of 'f' is 19\n",
            "Encoding of 'y' is 20\n",
            "Encoding of 'w' is 21\n",
            "Encoding of 'b' is 22\n",
            "Encoding of '.' is 23\n",
            "Encoding of 'v' is 24\n",
            "Encoding of 'k' is 25\n",
            "Encoding of 'S' is 26\n",
            "Encoding of ',' is 27\n",
            "Encoding of 'A' is 28\n",
            "Encoding of '-' is 29\n",
            "Encoding of 'T' is 30\n",
            "Encoding of 'C' is 31\n",
            "Encoding of 'P' is 32\n",
            "Encoding of 'M' is 33\n",
            "Encoding of 'I' is 34\n",
            "Encoding of ';' is 35\n",
            "Encoding of 'N' is 36\n",
            "Encoding of 'R' is 37\n",
            "Encoding of 'B' is 38\n",
            "Encoding of 'E' is 39\n",
            "Encoding of 'O' is 40\n",
            "Encoding of 'F' is 41\n",
            "Encoding of '3' is 42\n",
            "Encoding of 'W' is 43\n",
            "Encoding of 'D' is 44\n",
            "Encoding of '0' is 45\n",
            "Encoding of '9' is 46\n",
            "Encoding of 'U' is 47\n",
            "Encoding of 'L' is 48\n",
            "Encoding of 'x' is 49\n",
            "Encoding of 'G' is 50\n",
            "Encoding of 'H' is 51\n",
            "Encoding of '#' is 52\n",
            "Encoding of 'q' is 53\n",
            "Encoding of '1' is 54\n",
            "Encoding of '(' is 55\n",
            "Encoding of ')' is 56\n",
            "Encoding of '2' is 57\n",
            "Encoding of ''' is 58\n",
            "Encoding of 'K' is 59\n",
            "Encoding of 'J' is 60\n",
            "Encoding of '\\' is 61\n",
            "Encoding of '&' is 62\n",
            "Encoding of 'j' is 63\n",
            "Encoding of 'z' is 64\n",
            "Encoding of ':' is 65\n",
            "Encoding of '/' is 66\n",
            "Encoding of '5' is 67\n",
            "Encoding of '4' is 68\n",
            "Encoding of 'V' is 69\n",
            "Encoding of 'Y' is 70\n",
            "Encoding of '6' is 71\n",
            "Encoding of '\"' is 72\n",
            "Encoding of '7' is 73\n",
            "Encoding of '$' is 74\n",
            "Encoding of '8' is 75\n",
            "Encoding of '=' is 76\n",
            "Encoding of 'Q' is 77\n",
            "Encoding of '?' is 78\n",
            "Encoding of 'Z' is 79\n",
            "Encoding of 'X' is 80\n",
            "Encoding of '!' is 81\n",
            "Encoding of '_' is 82\n",
            "Encoding of '*' is 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the example of how we can encode the text from our dataset:"
      ],
      "metadata": {
        "id": "tDP_EkmtiY68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enc(x):\n",
        "    return torch.LongTensor(encode(x,voc=vocab,tokenizer=char_tokenizer))\n",
        "\n",
        "print(f'sample text:\\n{train_dataset[0][1]} ')\n",
        "print(f'\\nCharater encoding:\\n{enc(train_dataset[0][1])} ')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text:\n",
            "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again. \n",
            "\n",
            "Charater encoding:\n",
            "tensor([43,  4, 11, 11,  2, 26,  5, 23,  2, 38,  3,  4, 10,  9,  2, 31, 11,  4,\n",
            "        21,  2, 38,  4, 14, 25,  2, 34,  8,  5,  6,  2,  5, 13,  3,  2, 38, 11,\n",
            "         4, 14, 25,  2, 55, 37,  3, 15,  5,  3, 10,  9, 56,  2, 37,  3, 15,  5,\n",
            "         3, 10,  9,  2, 29,  2, 26, 13,  6, 10,  5, 29,  9,  3, 11, 11,  3, 10,\n",
            "         9, 27,  2, 43,  4, 11, 11,  2, 26,  5, 10,  3,  3,  5, 58,  9,  2, 12,\n",
            "        21,  7,  8, 12, 11,  7,  8, 18, 61, 22,  4,  8, 12,  2,  6, 19,  2, 15,\n",
            "        11,  5, 10,  4, 29, 14, 20,  8,  7, 14,  9, 27,  2,  4, 10,  3,  2,  9,\n",
            "         3,  3,  7,  8, 18,  2, 18, 10,  3,  3,  8,  2,  4, 18,  4,  7,  8, 23]) \n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "nFTMkLHWiY69",
        "outputId": "1ea66580-8c06-44f2-f8f1-6ba34ed0df5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a generative RNN\n",
        "\n",
        "The way we will train RNN to generate text is the following. On each step, we will take a sequence of characters of length `nchars`, and ask the network to generate the next output character for each input character:\n",
        "\n",
        "<img alt=\"Image showing an example RNN generation of the word 'HELLO'.\" src=\"images/6-generative-networks-2.png\" align=\"middle\" />\n",
        "\n",
        "Depending on the actual scenario, we may also want to include some special characters, such as *end-of-sequence* `<eos>`. In our case, we just want to train the network for endless text generation, thus we will fix the size of each sequence to be equal to `nchars` tokens. Consequently, each training example will consist of `nchars` inputs and `nchars` outputs (which are input sequence shifted one symbol to the left). Minibatch will consist of several such sequences.\n",
        "\n",
        "The way we will generate minibatches is to take each news text of length `l`, and generate all possible input-output combinations from it (there will be `l-nchars` such combinations). They will form one minibatch, and size of minibatches would be different at each training step. "
      ],
      "metadata": {
        "id": "EnPmaG4diY69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nchars = 100\n",
        "\n",
        "def get_batch(s,nchars=nchars):\n",
        "    ins = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
        "    outs = torch.zeros(len(s)-nchars,nchars,dtype=torch.long,device=device)\n",
        "    for i in range(len(s)-nchars):\n",
        "        ins[i] = enc(s[i:i+nchars])\n",
        "        outs[i] = enc(s[i+1:i+nchars+1])\n",
        "    return ins,outs\n",
        "\n",
        "get_batch(train_dataset[0][1])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[43,  4, 11,  ..., 18, 61, 22],\n",
              "         [ 4, 11, 11,  ..., 61, 22,  4],\n",
              "         [11, 11,  2,  ..., 22,  4,  8],\n",
              "         ...,\n",
              "         [37,  3, 15,  ...,  4, 18,  4],\n",
              "         [ 3, 15,  5,  ..., 18,  4,  7],\n",
              "         [15,  5,  3,  ...,  4,  7,  8]], device='cuda:0'),\n",
              " tensor([[ 4, 11, 11,  ..., 61, 22,  4],\n",
              "         [11, 11,  2,  ..., 22,  4,  8],\n",
              "         [11,  2, 26,  ...,  4,  8, 12],\n",
              "         ...,\n",
              "         [ 3, 15,  5,  ..., 18,  4,  7],\n",
              "         [15,  5,  3,  ...,  4,  7,  8],\n",
              "         [ 5,  3, 10,  ...,  7,  8, 23]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "NDDPeRMiiY69",
        "outputId": "3f64aafc-9097-4eb5-907e-30cf05c331f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the generative network. It can be based on any recurrent cell which we discussed in the previous unit (simple, LSTM or GRU). In our example we will use LSTM.\n",
        "\n",
        "Because the network takes characters as input, and vocabulary size is pretty small, we do not need embedding layer, one-hot-encoded input can directly go to LSTM cell. However, because we pass character numbers as input, we need to one-hot-encode them before passing to LSTM. This is done by calling `one_hot` function during `forward` pass. Output encoder would be a linear layer that will convert hidden state into one-hot-encoded output."
      ],
      "metadata": {
        "id": "sQJ-Ks3miY69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMGenerator(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.rnn = torch.nn.LSTM(vocab_size,hidden_dim,batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, s=None):\n",
        "        x = torch.nn.functional.one_hot(x,vocab_size).to(torch.float32)\n",
        "        x,s = self.rnn(x,s)\n",
        "        return self.fc(x),s"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "ytoluTuciY69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, we want to be able to sample generated text. To do that, we will define a `generate` function that will produce an output string of length `size`, starting from the initial string `start`.\n",
        "\n",
        "The way it works is the following:\n",
        "- First, we will pass the whole start string through the network, and take output state `s`\n",
        "and next predicted character `out`. \n",
        "- Since `out` is one-hot encoded, we take `argmax` to get the index of the character `nc` in the vocabulary, and use `itos` to figure out the actual character and append it to the resulting list of characters `chars`. \n",
        "- This process of generating one character is repeated `size` times to generate required number of characters.  "
      ],
      "metadata": {
        "id": "vGJ1p-WciY69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(net,size=100,start='today '):\n",
        "        chars = list(start)\n",
        "        out, s = net(enc(chars).view(1,-1).to(device))\n",
        "        for i in range(size):\n",
        "            nc = torch.argmax(out[0][-1])\n",
        "            chars.append(vocab.itos[nc])\n",
        "            out, s = net(nc.view(1,-1),s)\n",
        "        return ''.join(chars)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "iXrPrpsliY6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do the training! The training loop is almost the same as in all our previous examples, but instead of accuracy we print sampled generated text every 1000 epochs.\n",
        "\n",
        "Special attention needs to be paid to the way we compute loss. We need to compute loss given one-hot-encoded output `out`, and expected text `text_out`, which is the list of character indices. Luckily, the `cross_entropy` function expects unnormalized network output as first argument, and class number as the second, which is exactly what we have. It also performs automatic averaging over minibatch size.\n",
        "\n",
        "We also limit the training by `samples_to_train` samples, in order not to wait for too long. We encourage you to experiment and try longer training, possibly for several epochs (in which case you would need to create another loop around this code)."
      ],
      "metadata": {
        "id": "u8nYHmNoiY6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHJVLFDFMHb9",
        "outputId": "716b9198-02e4-4a0f-f250-5a256083aa37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = LSTMGenerator(vocab_size,64).to(device)\n",
        "\n",
        "samples_to_train = 20000\n",
        "optimizer = torch.optim.Adam(net.parameters(),0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "net.train()\n",
        "for i,x in enumerate(train_dataset):\n",
        "    # x[0] is class label, x[1] is text\n",
        "    if len(x[1])-nchars<10:\n",
        "        continue\n",
        "    samples_to_train-=1\n",
        "    if not samples_to_train: break\n",
        "    text_in, text_out = get_batch(x[1])\n",
        "    optimizer.zero_grad()\n",
        "    out,s = net(text_in)\n",
        "    loss = torch.nn.functional.cross_entropy(out.view(-1,vocab_size),text_out.flatten()) #cross_entropy(out,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i%1000==0:\n",
        "        print(f\"Current loss = {loss.item()}\")\n",
        "        print(generate(net))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current loss = 4.451519012451172\n",
            "today nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn\n",
            "Current loss = 2.104508399963379\n",
            "today and a the the the the the the the the the the the the the the the the the the the the the the the th\n",
            "Current loss = 1.6041269302368164\n",
            "today and the propper a dear to the US and a dear to the US and a dear to the US and a dear to the US and \n",
            "Current loss = 2.389401912689209\n",
            "today a set to the provition of the provition of the provition of the provition of the provition of the pr\n",
            "Current loss = 1.6965137720108032\n",
            "today and the first the first the first the first the first the first the first the first the first the fi\n",
            "Current loss = 1.6579563617706299\n",
            "today of the US the profits of the first the profits of the first the profits of the first the profits of \n",
            "Current loss = 1.9347177743911743\n",
            "today and a peasters and the first the first the first the first the first the first the first the first t\n",
            "Current loss = 1.8264572620391846\n",
            "today and a service and the seek and a service and the seek and a service and the seek and a service and t\n",
            "Current loss = 1.8270704746246338\n",
            "today and the company of the company of the company of the company of the company of the company of the co\n",
            "Current loss = 1.4838216304779053\n",
            "today and the security strend the security strend the security strend the security strend the security str\n",
            "Current loss = 1.611190676689148\n",
            "today and the be that the be that the be that the be that the be that the be that the be that the be that \n",
            "Current loss = 2.0964314937591553\n",
            "today and the security a strice a strice the security a strice a strice the security a strice a strice the\n",
            "Current loss = 1.8300071954727173\n",
            "today of the second to the second to the second to the second to the second to the second to the second to\n",
            "Current loss = 1.8434228897094727\n",
            "today and the set the set the set the set the set the set the set the set the set the set the set the set \n",
            "Current loss = 2.0300650596618652\n",
            "today of the security and the security and the security and the security and the security and the security\n",
            "Current loss = 1.877478003501892\n",
            "today and the computer to the security and the security and the security and the security and the security\n",
            "Current loss = 1.5313633680343628\n",
            "today and the season the season the season the season the season the season the season the season the seas\n",
            "Current loss = 1.7450498342514038\n",
            "today and the state and the state and the state and the state and the state and the state and the state an\n",
            "Current loss = 1.4873758554458618\n",
            "today and the state and the state and the state and the state and the state and the state and the state an\n",
            "Current loss = 1.588678002357483\n",
            "today and the state the state the state the state the state the state the state the state the state the st\n",
            "Current loss = 1.9452018737792969\n",
            "today and the state the start of the state the start of the state the start of the state the start of the \n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "json"
        },
        "id": "B4BEnVQqiY6-",
        "outputId": "2c19ab68-99ed-4c67-b5ce-2b1a7210b475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 用时5min"
      ],
      "metadata": {
        "id": "SkqcTDodOZ9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example already generates some pretty good text, but it can be further improved in several ways:\n",
        "* **Better minibatch generation**. The way we prepared data for training was to generate one minibatch from one sample. This is not ideal, because minibatches are all of different sizes, and some of them even cannot be generated, because the text is smaller than `nchars`. Also, small minibatches do not load GPU sufficiently enough. It would be wiser to get one large chunk of text from all samples, then generate all input-output pairs, shuffle them, and generate minibatches of equal size.\n",
        "* **Multilayer LSTM**. It makes sense to try 2 or 3 layers of LSTM cells. As we mentioned in the previous unit, each layer of LSTM extracts certain patterns from text, and in case of character-level generator we can expect lower LSTM level to be responsible for extracting syllables, and higher levels - for words and word combinations. This can be simply implemented by passing number-of-layers parameter to LSTM constructor.\n",
        "* You may also want to experiment with **GRU units** and see which ones perform better, and with **different hidden layer sizes**. Too large hidden layer may result in overfitting (e.g. network will learn exact text), and smaller size might not produce good result."
      ],
      "metadata": {
        "id": "NfmJ81zqiY6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft text generation and temperature\n",
        "\n",
        "In the previous definition of `generate`, we were always taking the character with highest probability as the next character in generated text. This resulted in the fact that the text often \"cycled\" between the same character sequences again and again, like in this example:\n",
        "```\n",
        "today of the second the company and a second the company ...\n",
        "```\n",
        "\n",
        "However, if we look at the probability distribution for the next character, it could be that the difference between a few highest probabilities is not huge, e.g. one character can have probability 0.2, another - 0.19, etc. For example, when looking for the next character in the sequence '*play*', next character can equally well be either space, or **e** (as in the word *player*).\n",
        "\n",
        "This leads us to the conclusion that it is not always \"fair\" to select the character with higher probability, because choosing the second highest might still lead us to meaningful text. It is more wise to **sample** characters from the probability distribution given by the network output.\n",
        "\n",
        "This sampling can be done using `multinomial` function that implements so-called **multinomial distribution**. A function that implements this **soft** text generation is defined below:"
      ],
      "metadata": {
        "id": "dcpLDFQeiY6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_soft(net,size=100,start='today ',temperature=1.0):\n",
        "        chars = list(start)\n",
        "        out, s = net(enc(chars).view(1,-1).to(device))\n",
        "        for i in range(size):\n",
        "            #nc = torch.argmax(out[0][-1])\n",
        "            out_dist = out[0][-1].div(temperature).exp()\n",
        "            nc = torch.multinomial(out_dist,1)[0]\n",
        "            chars.append(vocab.itos[nc])\n",
        "            out, s = net(nc.view(1,-1),s)\n",
        "        return ''.join(chars)\n",
        "    \n",
        "for i in [0.2,0.3,0.8,1.0,1.3,1.8]:\n",
        "    print(f\"--- Temperature = {i}\\n{generate_soft(net,size=300,start='Today ',temperature=i)}\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Temperature = 0.2\n",
            "Today on Monday of the state in the the strike to the stake and the stakes of the for the strong the weekend of the state in the stake and the become the stake of the propostical in the strent and the stages of the world to the state in the stakes in the state of the strong and the strong to the company o\n",
            "\n",
            "--- Temperature = 0.3\n",
            "Today Applay China in the country in the largest of the world to the company and the first the first the in the first the in to a storm and the stage of the strike in the enter the state of the make of the first the regulations of the postions on the new to a stakes and the new outside in a state to race \n",
            "\n",
            "--- Temperature = 0.8\n",
            "Today Brada Cup of the previoas of ching time agamed to former phay. American Blurborro President who short's survicoriers New York Recsing beamed company at the Sunday wounced of the elections in his the way with the somition of loss to New York Yed indioled down Krugh-base of the accuper development of \n",
            "\n",
            "--- Temperature = 1.0\n",
            "Today to Fronson Caliin The ares.\"nghac Pachican was Goas in and at the Aram to swace to ppes from training over streight meker, off undors up highned against the XP Jampojers.com) Cinniza Andran (AP) AP - traying about the schook, its eparenez party over four positing shut from the stex and Kambi beginni\n",
            "\n",
            "--- Temperature = 1.3\n",
            "Today on Western  Wanks #39;s 5,000 5-10 rum corent;wastamamper scaking theysesbay, a astash. wakerion froducing took get Friday GUin NFVT: Sli-banssing covedmage, Ilend With On after inssing 7-2 got of facter, Mailixcal technoli switery (Reuters) Nep. Zin people, a with has UKsep or Islairne secure,hes l\n",
            "\n",
            "--- Temperature = 1.8\n",
            "Today Wandbwakchz), hiky if'ighly agreed a fluad fannes at-tat MGXTR or 7BlnsGhoo\\Goneyi Hudn Even.LEEK #39;S NEWTEO Yuns pskges\" qualafy tiokbenso 1q7, 1.101F opposidfuh Cruckshurt, Morm Darhe Hus Offed, the haopaids,\" rade U.S\" kild gov.rrpaemeess after todrsovinglisaly's selcec'i brartiou over to, spur\n",
            "\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "json"
        },
        "id": "9VYqIb54iY6_",
        "outputId": "08fd4a65-82d1-4d7c-ef7e-25d23f0bfbb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have introduced one more parameter called **temperature**, which is used to indicate how hard we should stick to the highest probability. If temperature is 1.0, we do fair multinomial sampling, and when temperature goes to infinity - all probabilities become equal, and we randomly select next character. In the example below we can observe that the text becomes meaningless when we increase the temperature too much, and it resembles \"cycled\" hard-generated text when it becomes closer to 0. "
      ],
      "metadata": {
        "id": "SsgK-VeCiY6_"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}