{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Representing text\n",
        "\n",
        "If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors. Computers already represent textual characters as numbers that map to fonts on your screen using encodings such as ASCII or UTF-8.\n",
        "\n",
        "<img alt=\"Image showing diagram mapping a character to an ASCII and binary representation\" src=\"images/2-represent-text-as-tensor-checkpoint-1.png\" align=\"middle\" />\n",
        "\n",
        "We understand what each letter **represents**, and how all characters come together to form the words of a sentence. However, computers by themselves do not have such an understanding, and a neural network has to learn the meaning during training.\n",
        "\n",
        "Therefore, we can use different approaches when representing text:\n",
        "* **Character-level representation**, when we represent text by treating each character as a number. Given that we have $C$ different characters in our text corpus, the word *Hello* would be represented by $5\\times C$ tensor. Each letter would correspond to a tensor column in one-hot encoding.\n",
        "* **Word-level representation**, when we create a **vocabulary** of all words in our text sequence or sentence(s), and then represent each word using one-hot encoding. This approach is somehow better, because each letter by itself does not have much meaning, and thus by using higher-level semantic concepts - words - we simplify the task for the neural network. However, given a large dictionary size, we need to deal with high-dimensional sparse tensors.  For example, if we have a vocabulary size of 10,000 different words.  Then each word would have an one-hot encoding length of 10,000; hence the high-dimensional.\n",
        "\n",
        "To unify those approaches, we typically call an atomic piece of text **a token**. In some cases tokens can be letters, in other cases - words, or parts of words.\n",
        "\n",
        "> For example, we can choose to tokenize *indivisible* as `in`-`divis`-`ible`, where the `#` sign represents that the token is a continuation of the previous word. This would allow the root `divis` to always be represented by one token, corresponding to one core meaning.\n",
        "\n",
        "The process of converting text into a sequence of tokens is called **tokenization**. Next, we need to assign each token to a number, which we can feed into a neural network. This is called **vectorization**, and is normally done by building a token vocabulary.  \n",
        "\n",
        "Let's start by installing some required Python packages we'll use in this module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`requirement.txt` is as follows:\n",
        "\n",
        "```\n",
        "gensim==3.8.3\n",
        "huggingface==0.0.1\n",
        "matplotlib\n",
        "nltk==3.5\n",
        "numpy==1.18.5\n",
        "opencv-python==4.5.1.48\n",
        "Pillow==7.1.2\n",
        "scikit-learn\n",
        "scipy\n",
        "torch==1.8.1\n",
        "torchaudio==0.8.1\n",
        "torchinfo==0.0.8\n",
        "torchtext==0.9.1\n",
        "torchvision==0.9.1\n",
        "transformers==4.3.3\n",
        "```\n",
        "\n",
        "如果需要安装用下面命令即可\n",
        "\n",
        "```python\n",
        "!pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "对其中一些包功能进行解释：\n",
        "\n",
        "* `gensim`：用于处理文本数据的包，包含了一些文本处理的方法，比如词袋模型、TF-IDF、word2vec、LDA等\n",
        "* `huggingface`：提供很多预训练模型，比如BERT、GPT-2等\n",
        "* `matplotlib`：用于绘图的包\n",
        "* `nltk`：自然语言处理的包，包含了一些文本处理的方法，比如分词、词性标注、命名实体识别等\n",
        "* `numpy`：用于科学计算的包\n",
        "* `opencv-python`：用于图像处理的包\n",
        "* `Pillow`：用于图像处理的包\n",
        "* `scikit-learn`：用于机器学习的包\n",
        "* `scipy`：用于科学计算的包"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text classification task\n",
        "\n",
        "In this module, we will start with a simple text classification task based on **AG_NEWS** sample dataset, which is to classify news headlines into one of 4 categories: _World, Sports, Business and Sci/Tech_. This dataset is built from PyTorch's `torchtext` module, so we can easily access it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "os.makedirs('./data',exist_ok=True)\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, `train_dataset` and `test_dataset` contain iterators that return pairs of label (number of class) and text respectively, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.\")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, let's print out the first 5 new headlines from our dataset: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Sci/Tech** -> No Need for OPEC to Pump More-Iran Gov  TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when markets are already oversupplied by 2.8 million  barrels per day (bpd) of crude, Iran's OPEC governor said  Saturday, warning that prices could fall sharply.\n",
            "\n",
            "**Sci/Tech** -> Non-OPEC Nations Should Up Output-Purnomo  JAKARTA (Reuters) - Non-OPEC oil exporters should consider  increasing output to cool record crude prices, OPEC President  Purnomo Yusgiantoro said on Sunday.\n",
            "\n",
            "**Sci/Tech** -> Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (Reuters) - The auction for Google  Inc.'s highly anticipated initial public offering got off to a  rocky start on Friday after the Web search company sidestepped  a bullet from U.S. securities regulators.\n",
            "\n",
            "**Sci/Tech** -> Dollar Falls Broadly on Record Trade Gap  NEW YORK (Reuters) - The dollar tumbled broadly on Friday  after data showing a record U.S. trade deficit in June cast  fresh doubts on the economy's recovery and its ability to draw  foreign capital to fund the growing gap.\n",
            "\n",
            "**Sci/Tech** -> Rescuing an Old Saver If you think you may need to help your elderly relatives with their finances, don't be shy about having the money talk -- soon.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i,x in zip(range(5),train_dataset):\n",
        "    print(f\"**{classes[x[0]]}** -> {x[1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because datasets are iterators, if we want to use the data multiple times we need to convert it to a list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "train_dataset = list(train_dataset)\n",
        "test_dataset = list(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "Now we need to convert text into **numbers** that can be represented as tensors to feed them into a neural network. The first step is to convert text to tokens - **tokenization**. If we use word-level representation, each word would be represented by its own token. We will use build-in tokenizer from `torchtext` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use PyTorch's tokenizer to split words and spaces in the first 2 news articles. In our case, we use basic_english for the tokenizer to understand the language structure. This will return a string list of the text and characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "first token list:\n",
            "['wall', 'st', '.', 'bears', 'claw', 'back', 'into', 'the', 'black', '(', 'reuters', ')', 'reuters', '-', 'short-sellers', ',', 'wall', 'street', \"'\", 's', 'dwindling\\\\band', 'of', 'ultra-cynics', ',', 'are', 'seeing', 'green', 'again', '.']\n",
            "\n",
            "second token list:\n",
            "['carlyle', 'looks', 'toward', 'commercial', 'aerospace', '(', 'reuters', ')', 'reuters', '-', 'private', 'investment', 'firm', 'carlyle', 'group', ',', '\\\\which', 'has', 'a', 'reputation', 'for', 'making', 'well-timed', 'and', 'occasionally\\\\controversial', 'plays', 'in', 'the', 'defense', 'industry', ',', 'has', 'quietly', 'placed\\\\its', 'bets', 'on', 'another', 'part', 'of', 'the', 'market', '.']\n"
          ]
        }
      ],
      "source": [
        "first_sentence = train_dataset[0][1]\n",
        "second_sentence = train_dataset[1][1]\n",
        "\n",
        "f_tokens = tokenizer(first_sentence)\n",
        "s_tokens = tokenizer(second_sentence)\n",
        "\n",
        "print(f'\\nfirst token list:\\n{f_tokens}')\n",
        "print(f'\\nsecond token list:\\n{s_tokens}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, to convert text to numbers, we will need to build a vocabulary of all tokens. We first build the dictionary using the `Counter` object, and then create a `Vocab` object that would help us deal with vectorization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see how each word maps to the vocabulary, we'll loop through each word in the list to lookup it's index number in `vocab`.  Each word or character is displayed with it's corresponding index.  For example, word 'the' appears several times in both sentence and it's unique index in the vocab is the number 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Index lockup in 1st sentence:\n",
            "[[432, 'wall'], [426, 'st'], [2, '.'], [1606, 'bears'], [14839, 'claw'], [114, 'back'], [67, 'into'], [3, 'the'], [849, 'black'], [14, '('], [28, 'reuters'], [15, ')'], [28, 'reuters'], [16, '-'], [50726, 'short-sellers'], [4, ','], [432, 'wall'], [375, 'street'], [17, \"'\"], [10, 's'], [67508, 'dwindling\\\\band'], [7, 'of'], [52259, 'ultra-cynics'], [4, ','], [43, 'are'], [4010, 'seeing'], [784, 'green'], [326, 'again'], [2, '.']]\n",
            "\n",
            "Index lockup in 2nd sentence:\n",
            "[[15875, 'carlyle'], [1073, 'looks'], [855, 'toward'], [1311, 'commercial'], [4251, 'aerospace'], [14, '('], [28, 'reuters'], [15, ')'], [28, 'reuters'], [16, '-'], [930, 'private'], [798, 'investment'], [321, 'firm'], [15875, 'carlyle'], [99, 'group'], [4, ','], [27658, '\\\\which'], [29, 'has'], [6, 'a'], [4460, 'reputation'], [12, 'for'], [565, 'making'], [52791, 'well-timed'], [9, 'and'], [80618, 'occasionally\\\\controversial'], [2126, 'plays'], [8, 'in'], [3, 'the'], [526, 'defense'], [242, 'industry'], [4, ','], [29, 'has'], [3891, 'quietly'], [82815, 'placed\\\\its'], [6575, 'bets'], [11, 'on'], [207, 'another'], [360, 'part'], [7, 'of'], [3, 'the'], [127, 'market'], [2, '.']]\n"
          ]
        }
      ],
      "source": [
        "word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
        "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup}')\n",
        "\n",
        "word_lookup = [list((vocab[w], w)) for w in s_tokens]\n",
        "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using vocabulary, we can easily encode our tokenized string into a set of numbers. Let's use the first news article as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size if 95812\n",
            "[432, 426, 2, 1606, 14839, 114, 67, 3, 849, 14, 28, 15, 28, 16, 50726, 4, 432, 375, 17, 10, 67508, 7, 52259, 4, 43, 4010, 784, 326, 2]\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size if {vocab_size}\")\n",
        "\n",
        "def encode(x):\n",
        "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "vec = encode(first_sentence)\n",
        "print(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code, the torchtext `vocab.stoi` dictionary allows us to convert from a string representation into numbers (the name *stoi* stands for \"from **s**tring **to** **i**ntegers). To convert the text back from a numeric representation into text, we can use the `vocab.itos` dictionary to perform reverse lookup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wall',\n",
              " 'st',\n",
              " '.',\n",
              " 'bears',\n",
              " 'claw',\n",
              " 'back',\n",
              " 'into',\n",
              " 'the',\n",
              " 'black',\n",
              " '(',\n",
              " 'reuters',\n",
              " ')',\n",
              " 'reuters',\n",
              " '-',\n",
              " 'short-sellers',\n",
              " ',',\n",
              " 'wall',\n",
              " 'street',\n",
              " \"'\",\n",
              " 's',\n",
              " 'dwindling\\\\band',\n",
              " 'of',\n",
              " 'ultra-cynics',\n",
              " ',',\n",
              " 'are',\n",
              " 'seeing',\n",
              " 'green',\n",
              " 'again',\n",
              " '.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def decode(x):\n",
        "    return [vocab.itos[i] for i in x]\n",
        "\n",
        "decode(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiGrams, TriGrams and N-Grams\n",
        "\n",
        "One limitation of word tokenization is that some words are part of multi word expressions, for example, the word _'hot dog'_ has a completely different meaning than the words 'hot' and 'dog' in other contexts. If we represent words 'hot` and 'dog' always by the same vectors, it can confuse the model.\n",
        "\n",
        "To address this, **N-gram representations** are sometimes used in document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. \n",
        "- In **bigram** representation, for example, we will add all _word pairs_ to the vocabulary, in addition to original words. \n",
        "\n",
        "To get n-gram representation, we can use `ngrams_iterator` function that will convert the sequence of tokens to the sequence of n-grams. In the code below, we will build bigram vocabulary from our news dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram vocab size = 481971\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import ngrams_iterator\n",
        "\n",
        "bi_counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    bi_counter.update(ngrams_iterator(tokenizer(line),ngrams=2))\n",
        "bi_vocab = torchtext.vocab.Vocab(bi_counter, min_freq=2)\n",
        "\n",
        "print(f\"Bigram vocab size = {len(bi_vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[572,\n",
              " 564,\n",
              " 2,\n",
              " 2326,\n",
              " 49106,\n",
              " 150,\n",
              " 88,\n",
              " 3,\n",
              " 1143,\n",
              " 14,\n",
              " 32,\n",
              " 15,\n",
              " 32,\n",
              " 16,\n",
              " 443749,\n",
              " 4,\n",
              " 572,\n",
              " 499,\n",
              " 17,\n",
              " 10,\n",
              " 0,\n",
              " 7,\n",
              " 468770,\n",
              " 4,\n",
              " 52,\n",
              " 7019,\n",
              " 1050,\n",
              " 442,\n",
              " 2]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode(x):\n",
        "    return [bi_vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "encode(first_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main drawback of N-gram approach is that vocabulary size starts to grow extremely fast. Here we specify `min_freq` flag to `Vocab` constructor in order to avoid those tokens that appear in the text only once. We can also increase `min_freq` even further, because infrequent words/phrases usually have little effect on the accuracy of classification.\n",
        "\n",
        "> **Note:** Try setting set `min_freq` parameter to a higher value, and observe the length of vocabulary change.\n",
        "\n",
        "In practice, n-gram vocabulary size is still too high to represent words as one-hot vectors, and thus we need to combine this representation with some dimensionality reduction techniques, such as *embeddings*, which we will discuss in a later unit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feeding text into neural network\n",
        "\n",
        "We have learnt how to represent each word by a number. Now, to build text classification model, we need to feed the whole sentence (or whole news article) into a neural network. The problem here is that each article/sentence has variable length; and all fully-connected or convolution neural networks deal with fixed input size. There are two ways we can handle this problem:\n",
        "\n",
        "* Find a way to collapse a sentence into fixed-length vector. In the next unit we will see how **Bag-of-Words** and **TF-IDF** representations help us to do that.\n",
        "* Design special neural network architectures that can deal with variable length sequences. We'll learn how Recurrent neural networks (RNN) for sequence modeling are implemented later in this module."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
